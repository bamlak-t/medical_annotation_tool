{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f540c-2308-4b04-8902-ad64bec473e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain.chains import LLMChain\n",
    "from collections import namedtuple, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b02cec",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359a575-ec19-4743-8869-3f88c623b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: llama 2,  mistral, medllama2\n",
    "model = \"biomistral\"\n",
    "embedding_model = \"biomistral\"\n",
    "\n",
    "# taxonomy\n",
    "taxonomy_file_name = \"sirch_csv_1.txt\" # \"sirch_json_6m.txt\"\n",
    "\n",
    "# examples\n",
    "use_example_selector = False\n",
    "example_set_size = 2\n",
    "example_data_file_name = \"data.csv\"\n",
    "example_batch_size = 1\n",
    "num_examples = 2\n",
    "\n",
    "# input/output\n",
    "extract_index = 8\n",
    "prompt_index = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff36f1-7e87-43a6-b582-97ae0c6229d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "prompt_0 = '''Assume the role of a medical expert. Using the following taxonomy: {taxonomy} of human factors containing factors, subfactors and subsubfactors, I want you to find a list of the most relevant text segments to annotate in the following medical case: {text_extract}. Then for relevant text segments in the medical case, find the most fitting labels using the taxonomy to give the output as list of human factors in the following format: (\"text extract 1\": [\"factor1\", \"factor2\", ...], ...) and nothing else.'''\n",
    "# added \\n (line breaks) and used gpt4 to reword initial prompt\n",
    "prompt_1 = '''Imagine you are a medical expert. Given the following taxonomy:\\n {taxonomy}\\n of human factors containing factors, subfactors, and subsubfactors, your task is to find relevant text segments in this medical case:\\n {text_extract}\\n. Then, for each relevant text segment, identify the most fitting labels using the taxonomy. Present the output as a list of human factors in the following format: (\"text extract 1\": [\"factor1\", \"factor2\", ...], ...)'''\n",
    "# moved taxonomy to start\n",
    "prompt_2 = '''TAXONOMY: {taxonomy}\\n\\nImagine you are a medical expert. Using the taxonomy containing human factors containing factors, subfactors, and subsubfactors, your task is to find relevant text segments in the medical case:\\n {text_extract}\\n. Then, for each relevant text segment, identify the most fitting labels using the taxonomy. Present the output as a list of human factors in the following format: {{'TEXT_EXTRACT_1': [\"FACTOR_1\", \"FACTOR_2\", ...], ...}}  '''\n",
    "# moved medical extract to start\n",
    "prompt_3 = '''TAXONOMY: {taxonomy}\\n\\nMEDICAL CASE: {text_extract}\\n\\nImagine you are a medical expert. Using the taxonomy containing human factors containing factors, subfactors, and subsubfactors, your task is to find relevant text segments in the medical case. Then, for each relevant text segment, identify the most fitting labels using the taxonomy. Present the output as a list of human factors in the following format: {{'TEXT_EXTRACT_1': [\"FACTOR_1\", \"FACTOR_2\", ...], ...}}  '''\n",
    "# added output parser instead of manual format\n",
    "prompt_4 = '''TAXONOMY: {taxonomy}\\n\\nMEDICAL CASE: {text_extract}\\n\\nImagine you are a medical expert. Using the taxonomy containing human factors containing factors, your task is to find relevant text segments in the medical case. Then, for each relevant text segment, identify the most fitting labels using the taxonomy.\\n\\n{format_instructions}'''\n",
    "\n",
    "gpt_prompt_5 = '''TAXONOMY: {taxonomy}\\n\\nMEDICAL CASE: {text_extract}\\n\\nImagine you are a medical expert. Using the taxonomy containing human factors containing factors, your task is to find relevant text segments in the medical case. Then, for each relevant text segment, identify the most fitting labels using the taxonomy.\\n\\n{format_instructions}'''\n",
    "gpt_prompt_6 = '''TAXONOMY: {taxonomy}\\n\\nMEDICAL SCENARIO: {text_extract}\\n\\nEnvision yourself as a healthcare specialist. Your task is to locate relevant text segments in the medical scenario using the given taxonomy. Subsequently, label each segment with the most suitable category from the taxonomy.\\n\\n{format_instructions}'''\n",
    "gpt_prompt_7 = '''TAXONOMY: {taxonomy}\\n\\nMEDICAL TEXT: {text_extract}\\n\\nAssume the role of a medical expert. Your mission is to find and label relevant portions of the medical text using the taxonomy provided.\\n\\n{format_instructions}'''\n",
    "gpt_prompt_8 = '''TAXONOMY: {taxonomy}\\n\\nCASE DESCRIPTION: {text_extract}\\n\\nImagine being a medical practitioner. Your objective is to identify relevant text segments in the case description using the taxonomy. Then, assign the most fitting labels from the taxonomy to each segment.\\n\\n{format_instructions}'''\n",
    "gpt_prompt_9 = '''TAXONOMY: {taxonomy}\\n\\nMEDICAL REPORT: {text_extract}\\n\\nConsider yourself as a medical analyst. Your job is to find significant text segments in the medical report using the taxonomy. Then, label each segment with the most appropriate category from the taxonomy.\\n\\n{format_instructions}'''\n",
    "gpt_prompt_10 = '''Consider yourself as a medical analyst. Your job is to find significant text segments in the medical report using the taxonomy and definitions. Then, label each segment with the most appropriate category from the taxonomy.\\n\\nTAXONOMY:\\n\\n{taxonomy}\\n\\n{format_instructions}'''\n",
    "gpt_prompt_11 = '''Assume the role of a medical expert. Your mission is to find and label relevant portions of the medical text using the taxonomy provided.\\n\\nTAXONOMY:\\n\\n{taxonomy}\\n\\n{format_instructions}'''\n",
    "\n",
    "# test prompts\n",
    "\n",
    "inst = 'Annotate the text extract using the taxonomy.'\n",
    "\n",
    "test_prompt12 = inst + '\\n\\nTAXONOMY: {taxonomy}\\n\\n{format_instructions}'\n",
    "test_prompt13 = inst + '\\n\\n{format_instructions}\\n\\nTAXONOMY: {taxonomy}'\n",
    "\n",
    "test_prompt14 = 'TAXONOMY: {taxonomy}\\n\\n' + inst + '\\n\\n{format_instructions}'\n",
    "test_prompt15 = '{format_instructions}\\n\\n' + inst + '\\n\\nTAXONOMY: {taxonomy}'\n",
    "\n",
    "test_prompt16 = '{format_instructions}\\n\\nTAXONOMY: {taxonomy}\\n\\n' + inst \n",
    "test_prompt17 = 'TAXONOMY: {taxonomy}\\n\\n{format_instructions}\\n\\n' + inst\n",
    "\n",
    "test_prompts = [test_prompt12, test_prompt13, test_prompt14, test_prompt15, test_prompt16, test_prompt17]\n",
    "prompts = [prompt_0, prompt_1, prompt_2, prompt_3, prompt_4, gpt_prompt_5, gpt_prompt_6, gpt_prompt_7, gpt_prompt_8, gpt_prompt_9, gpt_prompt_10, gpt_prompt_11] + test_prompts\n",
    "prompt = prompts[prompt_index]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a724ff",
   "metadata": {},
   "source": [
    "### Loading taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = \"\"\n",
    "with open(taxonomy_file_name, 'r') as file:\n",
    "    taxonomy = file.read().replace('\\n', ' ')#.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5e59f",
   "metadata": {},
   "source": [
    "### Loading example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27560f04-0225-4723-90d6-c44cb3701ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(example_data_file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241d90e-f74b-4526-936b-241193953639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = df[[\"Code\", \"GPT rephrased sentence 1\"]].groupby('GPT rephrased sentence 1')['Code'].apply(list).reset_index(name='Labels')\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06158f1b",
   "metadata": {},
   "source": [
    "### Combine examples into batches\n",
    "\n",
    "This is because the data contains a single sentence mapped to a list of labels\n",
    "\n",
    "But there may be multiple sentences, each with their own list of labels.\n",
    "\n",
    "This batched data will map a list of sentences to a list of labels for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71b32e-629d-4599-ba49-2962eebcca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches = np.array_split(grouped, grouped.shape[0] / example_batch_size)\n",
    "\n",
    "# examples = []\n",
    "# for batch_index, batch in enumerate(batches[:example_set_size]):\n",
    "#     inp = \"\"\n",
    "#     out = \"\"    \n",
    "#     for index, item in batch.iterrows():\n",
    "#         inp += \" \" + item['GPT rephrased sentence 1']\n",
    "#         label = ', '.join(f'\"{i}\"' for i in item['Labels'])\n",
    "#         # out += ', \"' + item['GPT rephrased sentence 1'] +  '''\": [''' + label + ']'\n",
    "#         out += ', \"text_extract\": \"' + item['GPT rephrased sentence 1'] +  '''\",\\n\\t\"factors\": [''' + label + ']'\n",
    "\n",
    "#     # example = {\"input\": inp[1:], \"output\": \"{{\" + out[2:] + \"}}\"}\n",
    "#     # format as json\n",
    "#     example = {\"input\": inp[1:], \"output\": \"```json\\n{{\\n\\t\" + out[2:] + \"\\n}}\\n```\"}\n",
    "#     examples.append(example)\n",
    "\n",
    "\n",
    "# # print(examples[:2])\n",
    "# # print(examples[1][\"output\"])\n",
    "\n",
    "\n",
    "# examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56483da9",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47896da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a444ce5",
   "metadata": {},
   "source": [
    "### Output parser for parsing JSON from model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas([\n",
    "    ResponseSchema(\n",
    "        name='text_extract',\n",
    "        type='string',\n",
    "        description='Text extract from the medical case'\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name='factors',\n",
    "        type='List[string]',\n",
    "        description='List of factors associated with the text extract'\n",
    "    )\n",
    "])\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc070a-8cb5-4117-8fc8-eeb323ea80f8",
   "metadata": {},
   "source": [
    "### Create few shot template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e20b15-c517-4a9d-85ab-c53a567e91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [{'input': 'A MEOWS score was calculated on several occasions with an incomplete set of observation parameters being recorded.', 'output': '```json\\n{{\\n\\t\"text_extract\": \"A MEOWS score was calculated on several occasions with an incomplete set of observation parameters being recorded.\",\\n\\t\"factors\": [\"Assessment, investigation, testing, screening (e.g., holistic review)\"]\\n}}\\n```'}, {'input': 'A combination of handwritten and electronic antenatal healthcare records were used and not all of the mother s risk factors were highlighted in her electronic healthcare records.', 'output': '```json\\n{{\\n\\t\"text_extract\": \"A combination of handwritten and electronic antenatal healthcare records were used and not all of the mother s risk factors were highlighted in her electronic healthcare records.\",\\n\\t\"factors\": [\"Risk assessment\", \"Documentation\"]\\n}}\\n```'}]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "example_selector = None\n",
    "if use_example_selector:\n",
    "    example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "        examples, \n",
    "        OllamaEmbeddings(model=embedding_model), \n",
    "        Chroma, \n",
    "        k=num_examples\n",
    "    )\n",
    "\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    prefix=prompt,\n",
    "    suffix=\"Input: {text_extract}\\nOutput:\",\n",
    "    input_variables=[\"text_extract\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ") if use_example_selector else FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prompt,\n",
    "    suffix=\"Input: {text_extract}\\nOutput:\",\n",
    "    input_variables=[\"text_extract\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "print(similar_prompt.format(text_extract=\"INPUT WOULD GO HERE\", taxonomy=taxonomy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25028e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "parsed_results = []\n",
    "ChainOutput = namedtuple(\"ChainOutput\", \"avg_precision std_precision avg_recall std_recall runs_list bad_format_count\")\n",
    "\n",
    "def count_tp_fp_fn(output, expected):\n",
    "    tp, fp = defaultdict(int), defaultdict(int)\n",
    "    for factor in output[\"factors\"]:\n",
    "        if factor in expected:\n",
    "            tp[factor] +=1\n",
    "        else:\n",
    "            fp[factor] +=1\n",
    "\n",
    "    fn = len(expected) - len(tp)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def run_chain(chain, extract, expected, num_results, debug=False):\n",
    "    print(\"Expected:\", \" \".join(expected))\n",
    "    \n",
    "    bad_format_count = 0\n",
    "    runs_list = []\n",
    "    list_precision = []\n",
    "    list_recall = []\n",
    "\n",
    "    # making num_results number of inferences\n",
    "    for _ in range(num_results):\n",
    "        run_info = {}\n",
    "\n",
    "        # generate output using llm\n",
    "        llm_output = chain.invoke({\"text_extract\": extract, \"taxonomy\": taxonomy})\n",
    "        results.append(llm_output)\n",
    "\n",
    "        # parse generated output into json\n",
    "        parsed_output = None \n",
    "        try:\n",
    "            parsed_output = output_parser.parse(llm_output)\n",
    "            parsed_results.append(parsed_output)\n",
    "        except (TypeError, OutputParserException): \n",
    "            bad_format_count += 1\n",
    "            continue\n",
    "\n",
    "        run_info[\"expected_factors\"] = list(expected)\n",
    "        run_info[\"predicted_factors\"] = parsed_output[\"factors\"]\n",
    "\n",
    "        # count tp, fp and fn\n",
    "        tp, fp, fn_val = count_tp_fp_fn(output=parsed_output, expected=expected)\n",
    "        \n",
    "        tp_val = sum(tp.values())\n",
    "        fp_val = sum(fp.values())\n",
    "        \n",
    "        run_info[\"tp\"] = tp_val\n",
    "        run_info[\"fp\"] = fp_val\n",
    "        run_info[\"fn\"] = fn_val\n",
    "\n",
    "        # calculate precision and recall using tp, fp and fn\n",
    "        run_precision =  0 if (tp_val + fp_val) == 0 else tp_val / (tp_val + fp_val)\n",
    "        run_recall =  0 if (tp_val + fn_val) == 0 else tp_val / (tp_val + fn_val)\n",
    "        \n",
    "        run_info[\"precision\"] = run_precision\n",
    "        run_info[\"recall\"] = run_recall\n",
    "\n",
    "        # used to calculate avg and std\n",
    "        list_precision.append(run_precision)\n",
    "        list_recall.append(run_recall)\n",
    "\n",
    "        # keep track of runs\n",
    "        runs_list.append(run_info)\n",
    "\n",
    "        if debug:\n",
    "            print(\"----------------------------------------------------------\")\n",
    "            print(\"Run info:\", run_info)\n",
    "\n",
    "    \n",
    "    list_precision = np.array(list_precision)\n",
    "    list_recall = np.array(list_recall)\n",
    "\n",
    "    # calculating avg precision recall and std\n",
    "    avg_precision = list_precision.mean() if len(list_precision) > 0 else 0\n",
    "    std_precision = list_precision.std() if len(list_precision) > 0 else 0\n",
    "    avg_recall = list_recall.mean() if len(list_recall) > 0 else 0\n",
    "    std_recall = list_recall.std() if len(list_recall) > 0 else 0\n",
    " \n",
    "    if debug:\n",
    "        print(\"----------------------------------------------------------\")\n",
    "        print(\"Precision for input:\", avg_precision)\n",
    "        print(\"STD Precision for input:\", std_precision)\n",
    "        print(\"Recall for input:\", avg_recall)\n",
    "        print(\"STD Recall for input:\", std_recall)\n",
    "    \n",
    "    return ChainOutput(avg_precision, std_precision, avg_recall, std_recall, runs_list, bad_format_count)\n",
    "\n",
    "\n",
    "def save_json(f_name, dict):\n",
    "    with open(f_name, \"w\") as outfile: \n",
    "        json.dump(dict, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a986bd-7c9d-49a7-9283-d5f363e3ff86",
   "metadata": {},
   "source": [
    "### Running the chain\n",
    "\n",
    "Looping through each text extract, and getting average precision and recall for each class found in the extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df10714-e968-4883-bdec-579193e7aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first two used as examples\n",
    "offset = 2\n",
    "input_extracts = grouped[\"GPT rephrased sentence 1\"].tolist()[offset:]\n",
    "expected_codes = grouped[\"Labels\"].tolist()[offset:]\n",
    "chain = similar_prompt | llm\n",
    "\n",
    "# parameters\n",
    "num_results = 10\n",
    "num_of_extracts = 5\n",
    "\n",
    "# for printing and saving output as a json\n",
    "debug = 0\n",
    "\n",
    "# to measure prompt's ability to instruct the llm to generate a json output\n",
    "bad_format_count_total = 0\n",
    "\n",
    "# for calculating avg and std across multiple text extracts\n",
    "list_precision = []\n",
    "list_precision_std = []\n",
    "list_recall = []\n",
    "list_recall_std = []\n",
    "\n",
    "# sentence: parsed_output, expected, tp, fp, fn, precision, recall\n",
    "debug_output = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "''' \n",
    "    Calculate precision, recall and std for num_of_extracts each extract \n",
    "    for each extract, generate num_results outputs and keep track of run metrics\n",
    "'''\n",
    "for input_extract, expected in zip(input_extracts[:num_of_extracts], expected_codes[:num_of_extracts]):\n",
    "    print(\"Input:\", input_extract)\n",
    "    chain_output = run_chain(chain=chain, extract=input_extract, expected=expected, num_results=num_results, debug=debug)\n",
    "    print(\"Precision:\", chain_output.avg_precision, \"Recall:\", chain_output.avg_recall, \"\\n\")\n",
    "    \n",
    "    debug_output[input_extract] = {\"runs\": chain_output.runs_list, \n",
    "                                   \"avg_precision\": chain_output.avg_precision, \n",
    "                                   \"std_precision\": chain_output.std_precision, \n",
    "                                   \"avg_recall\": chain_output.avg_recall, \n",
    "                                   \"std_recall\": chain_output.std_recall,\n",
    "                                   \"bad_format_count\": chain_output.bad_format_count}\n",
    "    \n",
    "    bad_format_count_total += chain_output.bad_format_count\n",
    "\n",
    "    list_precision.append(chain_output.avg_precision)\n",
    "    list_precision_std.append(chain_output.std_precision)\n",
    "    list_recall.append(chain_output.avg_recall)\n",
    "    list_recall_std.append(chain_output.std_recall)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "if debug:\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"Debug output\\n\", debug_output)\n",
    "          \n",
    "save_json(\"debug.json\", dict(debug_output))\n",
    "\n",
    "list_precision_np = np.array(list_precision)\n",
    "list_precision_std_np = np.array(list_precision_std)\n",
    "list_recall_np = np.array(list_recall)\n",
    "list_recall_std_np = np.array(list_recall_std)\n",
    "\n",
    "print(\"Avg precision:\", list_precision_np.mean(), \n",
    "      \"| STD precision:\", list_precision_std_np.mean(), \n",
    "      \"| Avg recall:\", list_recall_np.mean(), \n",
    "      \"| STD recall:\", list_recall_std_np.mean())\n",
    "print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "print(\"Badly formatted jsons:\", bad_format_count_total, \"/\", len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c70002-c590-4fed-850f-910d72d7836e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Results plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29ad3b-2c34-4a56-80bf-37abeb23f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Distribution of list_precision_np\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(list_precision_np, bins=10, color='blue')\n",
    "plt.title('Distribution of list_precision_np')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Distribution of list_precision_std_np\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(list_precision_std_np, bins=10, color='green')\n",
    "plt.title('Distribution of list_precision_std_np')\n",
    "plt.xlabel('Precision STD')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Distribution of list_recall_np\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(list_recall_np, bins=10, color='red')\n",
    "plt.title('Distribution of list_recall_np')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Distribution of list_recall_std_np\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(list_recall_std_np, bins=10, color='orange')\n",
    "plt.title('Distribution of list_recall_std_np')\n",
    "plt.xlabel('Recall STD')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d077be-c991-4905-ab7c-db10b135bced",
   "metadata": {},
   "source": [
    "### All results parsed as JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c64b88-c3a1-47a3-b2d3-d114bdcc60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0419b14-c159-4c2c-86b7-0d8f5371dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "total_ratio = 0\n",
    "for i in results:\n",
    "    for j in results:\n",
    "        total_ratio += SequenceMatcher(None, i, j).ratio()\n",
    "\n",
    "# aveage result similarity across 10 inferences\n",
    "total_ratio/(len(results)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925fb0b-a359-4a3f-9aa3-7d3cfe9de237",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(\"output.json\", parsed_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
